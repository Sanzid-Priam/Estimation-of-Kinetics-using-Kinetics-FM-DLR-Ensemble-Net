{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "R5Chqe0Fg64I"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNOEHDaAbwZti9jJMBfbyEK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanzid-Priam/Estimation-of-Kinetics-using-Kinetics-FM-DLR-Ensemble-Net/blob/main/Multi_modal_Public_Dataset_2_kinetics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "import statistics \n",
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU,LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statistics import stdev \n",
        "import math\n",
        "import h5py\n",
        " \n",
        "import numpy as np\n",
        "\n",
        "from scipy.signal import butter,filtfilt\n",
        " \n",
        "import sys \n",
        "import numpy as np # linear algebra\n",
        "from scipy.stats import randint\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph \n",
        "import seaborn as sns # used for plot interactive graph. \n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "## for Deep-learing:\n",
        "import tensorflow.keras\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "to_categorical([0, 1, 2, 3], num_classes=4)\n",
        "from tensorflow.keras.optimizers import SGD \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# from tensorflow.keras.utils import np_utils\n",
        "import itertools\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "#import constraint\n",
        " \n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.regularizers import l2\n",
        " \n",
        " \n",
        "###  Library for attention layers \n",
        " \n",
        "import pandas as pd\n",
        "#import pyarrow.parquet as pq # Used to read the data\n",
        "import os \n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
        "from tensorflow.keras.models import Model\n",
        "#from tqdm import tqdm # Processing time measurement\n",
        "from sklearn.model_selection import train_test_split \n",
        "from tensorflow.keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
        "from tensorflow.keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
        "from tensorflow.keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting\n",
        " \n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "import statistics\n",
        "import gc\n",
        "\n",
        "\n",
        " \n",
        "### Early stopping \n",
        " \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        " \n",
        "\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H__KTa0RNQDo",
        "outputId": "860536ca-2990-4868-936c-d61d8564c00c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 16607452837286134994\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14417788928\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 3266013214621100893\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File path\n"
      ],
      "metadata": {
        "id": "TrXnoV0Js5CO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loader"
      ],
      "metadata": {
        "id": "3mq0s3oBqlJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    with h5py.File('/content/drive/My Drive/public dataset/all_17_subjects.h5', 'r') as hf:\n",
        "        data_all_sub = {subject: subject_data[:] for subject, subject_data in hf.items()}\n",
        "        data_fields = json.loads(hf.attrs['columns'])"
      ],
      "metadata": {
        "id": "TP90JbQoR23t"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_extraction(A):\n",
        "  for k in range(len(A)):\n",
        "    zero_index_1=np.all(A[k:k+1,:,:] == 0, axis=0)\n",
        "    zero_index = np.multiply(zero_index_1, 1)\n",
        "    zero_index=np.array(zero_index)\n",
        "\n",
        "    for i in range(len(zero_index)):\n",
        "      if (sum(zero_index[i])==256):\n",
        "        index=i\n",
        "        break;\n",
        "    # print(index)\n",
        "    B=A[k:k+1,20:index,:]\n",
        "    C_1=B.reshape((B.shape[0]*B.shape[1],B.shape[2]))\n",
        "    if (k==0):\n",
        "      C=C_1\n",
        "    else:\n",
        "      C=np.append(C,C_1,axis=0)\n",
        "\n",
        "  index_1 = data_fields.index('AccelX_R_FOOT')\n",
        "  index_2 = data_fields.index('AccelY_R_FOOT')\n",
        "  index_3 = data_fields.index('AccelZ_R_FOOT')\n",
        "  index_4 = data_fields.index('GyroX_R_FOOT')\n",
        "  index_5 = data_fields.index('GyroY_R_FOOT')\n",
        "  index_6 = data_fields.index('GyroZ_R_FOOT')\n",
        "  index_7 = data_fields.index('AccelX_R_SHANK')\n",
        "  index_8 = data_fields.index('AccelY_R_SHANK')\n",
        "  index_9 = data_fields.index('AccelZ_R_SHANK')\n",
        "  index_10 = data_fields.index('GyroX_R_SHANK')\n",
        "  index_11 = data_fields.index('GyroY_R_SHANK')\n",
        "  index_12 = data_fields.index('GyroZ_R_SHANK')\n",
        "  index_13 = data_fields.index('AccelX_R_THIGH')\n",
        "  index_14 = data_fields.index('AccelY_R_THIGH')\n",
        "  index_15 = data_fields.index('AccelZ_R_THIGH')\n",
        "  index_16 = data_fields.index('GyroX_R_THIGH')\n",
        "  index_17= data_fields.index('GyroY_R_THIGH')\n",
        "  index_18 = data_fields.index('GyroZ_R_THIGH')\n",
        "  index_19 = data_fields.index('EXT_KM_X')\n",
        "  index_20 = data_fields.index('EXT_KM_Y')\n",
        "  index_21 = data_fields.index('plate_2_force_x')\n",
        "  index_22 = data_fields.index('plate_2_force_y')\n",
        "  index_23 = data_fields.index('plate_2_force_z')\n",
        "  index_24 = data_fields.index('body weight')\n",
        "  index_25 = data_fields.index('body height')\n",
        "\n",
        "  BW=(C[0:1, index_24]*9.8)\n",
        "  BWH=(C[0:1, index_24]*9.8)*C[:, index_25]\n",
        "\n",
        "  D=np.vstack((C[:, index_1],C[:, index_2],C[:, index_3],C[:, index_4],C[:, index_5],C[:, index_6],C[:, index_7],C[:, index_8],\n",
        "              C[:, index_9],C[:, index_10],C[:, index_11],C[:, index_12],C[:, index_13],C[:, index_14]\n",
        "              ,C[:, index_15],C[:, index_16],C[:, index_17],C[:, index_18],C[:, index_19],C[:, index_20],-C[:, index_21]/BW,\n",
        "              -C[:, index_23]/BW,-C[:, index_22]/BW)).T\n",
        "\n",
        "\n",
        "  D=np.hstack((D,C[:,109:153]))\n",
        "\n",
        "  return D"
      ],
      "metadata": {
        "id": "0cUpq5GubU3l"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index_23 = data_fields.index('LShoulder_x_90')\n",
        "# print(index_23)\n"
      ],
      "metadata": {
        "id": "irvutxz06Hoo"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(np.array(data_fields))"
      ],
      "metadata": {
        "id": "C1LzVbey4AMX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_subject_01 = data_all_sub['subject_01']\n",
        "data_subject_02 = data_all_sub['subject_02']\n",
        "data_subject_03 = data_all_sub['subject_03']\n",
        "data_subject_04 = data_all_sub['subject_04']\n",
        "data_subject_05 = data_all_sub['subject_05']\n",
        "data_subject_06 = data_all_sub['subject_06']\n",
        "data_subject_07 = data_all_sub['subject_07']\n",
        "data_subject_08 = data_all_sub['subject_08']\n",
        "data_subject_09 = data_all_sub['subject_09']\n",
        "data_subject_10 = data_all_sub['subject_10']\n",
        "data_subject_11 = data_all_sub['subject_11']\n",
        "data_subject_12 = data_all_sub['subject_12']\n",
        "data_subject_13 = data_all_sub['subject_13']\n",
        "data_subject_14 = data_all_sub['subject_14']\n",
        "data_subject_15 = data_all_sub['subject_15']\n",
        "data_subject_16 = data_all_sub['subject_16']\n",
        "data_subject_17 = data_all_sub['subject_17']\n",
        "\n",
        "\n",
        "subject_1=data_extraction(data_subject_01)\n",
        "subject_2=data_extraction(data_subject_02)\n",
        "subject_3=data_extraction(data_subject_03)\n",
        "subject_4=data_extraction(data_subject_04)\n",
        "subject_5=data_extraction(data_subject_05)\n",
        "subject_6=data_extraction(data_subject_06)\n",
        "subject_7=data_extraction(data_subject_07)\n",
        "subject_8=data_extraction(data_subject_08)\n",
        "subject_9=data_extraction(data_subject_09)\n",
        "subject_10=data_extraction(data_subject_10)\n",
        "subject_11=data_extraction(data_subject_11)\n",
        "subject_12=data_extraction(data_subject_12)\n",
        "subject_13=data_extraction(data_subject_13)\n",
        "subject_14=data_extraction(data_subject_14)\n",
        "subject_15=data_extraction(data_subject_15)\n",
        "subject_16=data_extraction(data_subject_16)\n",
        "subject_17=data_extraction(data_subject_17)\n"
      ],
      "metadata": {
        "id": "Pwo6ALnFONNS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subject_1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsCxXC1B-JXc",
        "outputId": "36ea9c9c-37cb-4e56-e486-ea006d2b76ad"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67150, 67)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing and model Training"
      ],
      "metadata": {
        "id": "zM9iGjQ6uXU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = \"/content/drive/My Drive/public dataset/Public_dataset_2/Subject01\"\n",
        "# os.mkdir(main_dir) \n",
        "path=\"/content/drive/My Drive/public dataset/Public_dataset_2/Subject01/\"\n",
        "subject='Subject_01'"
      ],
      "metadata": {
        "id": "8fTO4veYsyC7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=np.concatenate((subject_2,subject_3,subject_4,subject_5,subject_6,subject_7,subject_8,subject_9,\n",
        "                              subject_10,subject_11,subject_12,subject_13,subject_14,subject_15,subject_16,subject_17),axis=0)\n",
        "\n",
        "test_dataset=subject_1"
      ],
      "metadata": {
        "id": "bvt8kQG5iLMP"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train features #\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "x_train_1=train_dataset[:,0:18]\n",
        "x_train_2=train_dataset[:,23:67]\n",
        "\n",
        "x_train=np.concatenate((x_train_1,x_train_2),axis=1)\n",
        "\n",
        "scale= StandardScaler()\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "train_X_1_1=x_train\n",
        " \n",
        "# # Test features #\n",
        "x_test_1=test_dataset[:,0:18]\n",
        "x_test_1=test_dataset[:,23:67]\n",
        "test_X_1_1=x_test\n",
        "\n",
        "\n",
        "  ### Label ###\n",
        "\n",
        "train_y_1_1=train_dataset[:,18:23]\n",
        "test_y_1_1=test_dataset[:,18:23]\n",
        "\n",
        "train_dataset_1=np.concatenate((train_X_1_1,train_y_1_1),axis=1)\n",
        "test_dataset_1=np.concatenate((test_X_1_1,test_y_1_1),axis=1)\n",
        "\n",
        "train_dataset_1=pd.DataFrame(train_dataset_1)\n",
        "test_dataset_1=pd.DataFrame(test_dataset_1)\n",
        "\n",
        "train_dataset_1.dropna(axis=0,inplace=True)\n",
        "test_dataset_1.dropna(axis=0,inplace=True)\n",
        "\n",
        "train_dataset_1=np.array(train_dataset_1)\n",
        "test_dataset_1=np.array(test_dataset_1)\n",
        "\n",
        "train_dataset_sum = np. sum(train_dataset_1)\n",
        "array_has_nan = np. isnan(train_dataset_sum)\n",
        "\n",
        "print(array_has_nan)\n",
        "\n",
        "print(train_dataset_1.shape)\n",
        "\n",
        "\n",
        "\n",
        "train_X_1=train_dataset_1[:,0:18]\n",
        "test_X_1=test_dataset_1[:,0:18]\n",
        "\n",
        "train_y_1=train_dataset_1[:,18:23]\n",
        "test_y_1=test_dataset_1[:,18:23]\n",
        "\n",
        "\n",
        "\n",
        "L1=len(train_X_1)\n",
        "L2=len(test_X_1)\n",
        "\n",
        "print(L1+L2)\n",
        " \n",
        "w=50\n",
        "\n",
        "                   \n",
        " \n",
        "a1=L1//w\n",
        "b1=L1%w\n",
        " \n",
        "a2=L2//w\n",
        "b2=L2%w\n",
        "\n",
        "# a3=L3//w\n",
        "# b3=L3%w \n",
        " \n",
        "     #### Features ####\n",
        "train_X_2=train_X_1[L1-w+b1:L1,:]\n",
        "test_X_2=test_X_1[L2-w+b2:L2,:]\n",
        "# validation_X_2=validation_X_1[L3-w+b3:L3,:]\n",
        " \n",
        "\n",
        "    #### Output ####\n",
        " \n",
        "train_y_2=train_y_1[L1-w+b1:L1,:]\n",
        "test_y_2=test_y_1[L2-w+b2:L2,:]\n",
        "# validation_y_2=validation_y_1[L3-w+b3:L3,:]\n",
        "\n",
        "\n",
        " \n",
        "     #### Features ####\n",
        "    \n",
        "train_X=np.concatenate((train_X_1,train_X_2),axis=0)\n",
        "test_X=np.concatenate((test_X_1,test_X_2),axis=0)\n",
        "# validation_X=np.concatenate((validation_X_1,validation_X_2),axis=0)\n",
        " \n",
        " \n",
        "    #### Output ####\n",
        "    \n",
        "train_y=np.concatenate((train_y_1,train_y_2),axis=0)\n",
        "test_y=np.concatenate((test_y_1,test_y_2),axis=0)\n",
        "# validation_y=np.concatenate((validation_y_1,validation_y_2),axis=0)\n",
        "\n",
        "    \n",
        "print(train_y.shape) \n",
        "    #### Reshaping ####\n",
        "train_X_3_p= train_X.reshape((a1+1,w,train_X.shape[1]))\n",
        "test_X = test_X.reshape((a2+1,w,test_X.shape[1]))\n",
        "\n",
        "\n",
        "train_y_3_p= train_y.reshape((a1+1,w,5))\n",
        "test_y= test_y.reshape((a2+1,w,5))\n",
        "\n",
        " \n",
        "\n",
        "# train_X_1D=train_X_3\n",
        "test_X_1D=test_X\n",
        "\n",
        "train_X_3=train_X_3_p\n",
        "train_y_3=train_y_3_p\n",
        "# print(train_X_4.shape,train_y_3.shape)\n",
        "\n",
        "\n",
        "train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)\n",
        "#train_X_1D, X_validation_1D_ridge, train_y, Y_validation_ridge = train_test_split(train_X_1D_m,train_y_m, test_size=0.10, random_state=True)   [0:2668,:,:]\n",
        "\n",
        "print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)\n",
        "\n",
        "features=6\n",
        "\n",
        "train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,3)\n",
        "test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,3)\n",
        "X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,3)\n",
        "#X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)\n",
        "\n",
        "\n",
        "print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)\n",
        "\n",
        "import tensorflow as tf\n",
        "# tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "Bag_samples=train_X_2D.shape[0]\n",
        "print(Bag_samples)\n",
        "\n",
        "s=test_X_1D.shape[0]*w\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7U-luFUh3gy",
        "outputId": "a1635302-38c0-492c-fda6-79cc83d3e56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "(984864, 23)\n",
            "1048716\n",
            "(984900, 5)\n",
            "(15758, 50, 18) (15758, 50, 5) (3940, 50, 18) (3940, 50, 5)\n",
            "(15758, 50, 6, 3) (1278, 50, 6, 3) (3940, 50, 6, 3)\n",
            "15758\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y_plot=test_y.reshape(test_y.shape[0]*test_y.shape[1],test_y.shape[2])\n",
        "test_y_plot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51BhKVkUbpGe",
        "outputId": "07b56305-be97-48b3-99b5-cc9aff097ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63900, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y_plot=train_y_3.reshape(train_y_3.shape[0]*train_y_3.shape[1],train_y_3.shape[2])\n",
        "train_y_plot.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tb2C4Eodzqu",
        "outputId": "fdc207c9-041d-4d3f-a0ce-c862ee88f33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(984900, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_y_plot[15044:15115,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "BR2Bd8mWeBW8",
        "outputId": "04ea13b6-1a80-43e2-df19-d4e5dad9258b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff05a620c10>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_y_plot[0:77,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "mOI-zzh0aMc4",
        "outputId": "2bec68f9-298b-414a-b4f3-bf4a24fa594a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff05a1458d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Functions"
      ],
      "metadata": {
        "id": "iP_eDnJwrKCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import pickle\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.utils import resample\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\"\"\"# Loss Function\"\"\"\n",
        "\n",
        "from keras import backend as K\n",
        "def correlation_coefficient_loss(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x)\n",
        "    my = K.mean(y)\n",
        "    xm, ym = x-mx, y-my\n",
        "    r_num = K.sum(tf.multiply(xm,ym))\n",
        "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "    r = r_num / r_den\n",
        "\n",
        "    # r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "\n",
        "    l1=K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "    #l2=1-K.square(r)\n",
        "    l2=1-r\n",
        "\n",
        "    l=l2\n",
        "    return l\n",
        "\n",
        "from keras import backend as K\n",
        "def correlation_coefficient_loss_1(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x)\n",
        "    my = K.mean(y)\n",
        "    xm, ym = x-mx, y-my\n",
        "    r_num = K.sum(tf.multiply(xm,ym))\n",
        "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "    r = r_num / r_den\n",
        "\n",
        "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "\n",
        "    l1=K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "    l2=1-K.square(r)\n",
        "\n",
        "    l=l1\n",
        "    return l\n",
        "\n",
        "from keras import backend as K\n",
        "def correlation_coefficient_loss_joint(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x)\n",
        "    my = K.mean(y)\n",
        "    xm, ym = x-mx, y-my\n",
        "    r_num = K.sum(tf.multiply(xm,ym))\n",
        "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "    r = r_num / r_den\n",
        "\n",
        "    #r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "\n",
        "    l1=K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "    #l2=1-K.square(r)\n",
        "    l2=1-r\n",
        "\n",
        "    l=l1+l2\n",
        "    return l\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def RMSE_prediction(yhat_4,test_y,s):\n",
        " \n",
        "  test_o=test_y.reshape((s,5))\n",
        "  yhat=yhat_4.reshape((s,5))\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  y_1_no=yhat[:,0]\n",
        "  y_2_no=yhat[:,1]\n",
        "  y_3_no=yhat[:,2]\n",
        "  y_4_no=yhat[:,3]\n",
        "  y_5_no=yhat[:,4]\n",
        "  # y_6_no=yhat[:,5]\n",
        "  # y_7_no=yhat[:,6]\n",
        "  #y_8_no=yhat[:,7]\n",
        "  #y_9_no=yhat[:,8]\n",
        "  #y_10_no=yhat[:,9]\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  y_test_1=test_o[:,0]\n",
        "  y_test_2=test_o[:,1]\n",
        "  y_test_3=test_o[:,2]\n",
        "  y_test_4=test_o[:,3]\n",
        "  y_test_5=test_o[:,4]\n",
        "  # y_test_6=test_o[:,5]\n",
        "  # y_test_7=test_o[:,6]\n",
        "  #y_test_8=test_o[:,7]\n",
        "  #y_test_9=test_o[:,8]\n",
        "  #y_test_10=test_o[:,9]\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  #print(y_1.shape,y_test_1.shape)\n",
        "  \n",
        "  \n",
        "  \n",
        "  cutoff=6\n",
        "  fs=200\n",
        "  order=4\n",
        "  \n",
        "  nyq = 0.5 * fs\n",
        "  ## filtering data ##\n",
        "  def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "      normal_cutoff = cutoff / nyq\n",
        "      # Get the filter coefficients \n",
        "      b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "      y = filtfilt(b, a, data)\n",
        "      return y\n",
        "  \n",
        "  \n",
        "  \n",
        "  y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)\n",
        "  y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)\n",
        "  y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)\n",
        "  y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)\n",
        "  y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)\n",
        "  # y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)\n",
        "  # y_7=butter_lowpass_filter(y_7_no, cutoff, fs, order)\n",
        "  #y_8=butter_lowpass_filter(y_8_no, cutoff, fs, order)\n",
        "  #y_9=butter_lowpass_filter(y_9_no, cutoff, fs, order)\n",
        "  #y_10=butter_lowpass_filter(y_10_no, cutoff, fs, order)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  Z_1=y_1\n",
        "  Z_2=y_2\n",
        "  Z_3=y_3\n",
        "  Z_4=y_4\n",
        "  Z_5=y_5\n",
        "  # Z_6=y_6\n",
        "  # Z_7=y_7\n",
        "  #Z_8=y_8\n",
        "  #Z_9=y_9\n",
        "  #Z_10=y_10\n",
        "  \n",
        "  \n",
        "  \n",
        "  ###calculate RMSE\n",
        "  \n",
        "  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100\n",
        "  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100\n",
        "  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100\n",
        "  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100\n",
        "  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100\n",
        "  # rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100\n",
        "  # rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100\n",
        "  #rmse_8 =((np.sqrt(mean_squared_error(y_test_8,y_8)))/(max(y_test_8)-min(y_test_8)))*100\n",
        "  #rmse_9 =((np.sqrt(mean_squared_error(y_test_9,y_9)))/(max(y_test_9)-min(y_test_9)))*100\n",
        "  #rmse_10 =((np.sqrt(mean_squared_error(y_test_10,y_10)))/(max(y_test_10)-min(y_test_10)))*100\n",
        "  \n",
        "  \n",
        "  print(rmse_1)\n",
        "  print(rmse_2)\n",
        "  print(rmse_3)\n",
        "  print(rmse_4)\n",
        "  print(rmse_5)\n",
        "  # print(rmse_6)\n",
        "  # print(rmse_7)\n",
        "  #print(rmse_8)\n",
        "  #print(rmse_9)\n",
        "  #print(rmse_10)\n",
        "  \n",
        "  \n",
        "  p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "  p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "  p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "  p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "  p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "  # p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "  # p_7=np.corrcoef(y_7, y_test_7)[0, 1]\n",
        "  #p_8=np.corrcoef(y_8, y_test_8)[0, 1]\n",
        "  #p_9=np.corrcoef(y_9, y_test_9)[0, 1]\n",
        "  #p_10=np.corrcoef(y_10, y_test_10)[0, 1]\n",
        "  \n",
        "  \n",
        "  print(\"\\n\") \n",
        "  print(p_1)\n",
        "  print(p_2)\n",
        "  print(p_3)\n",
        "  print(p_4)\n",
        "  print(p_5)\n",
        "  # print(p_6)\n",
        "  # print(p_7)\n",
        "  #print(p_8)\n",
        "  #print(p_9)\n",
        "  #print(p_10)\n",
        "  \n",
        "  \n",
        "              ### Correlation ###\n",
        "  p=np.array([p_1,p_2,p_3,p_4,p_5])\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "      #### Mean and standard deviation ####\n",
        "  \n",
        "  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])\n",
        "  \n",
        "      #### Mean and standard deviation ####\n",
        "  m=statistics.mean(rmse)\n",
        "  SD=statistics.stdev(rmse)\n",
        "  print('Mean: %.3f' % m,'+/- %.3f' %SD)\n",
        "   \n",
        "  m_c=statistics.mean(p)\n",
        "  SD_c=statistics.stdev(p)\n",
        "  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n",
        "\n",
        "  return rmse, p, Z_1,Z_2,Z_3,Z_4,Z_5\n",
        "\n",
        "\n",
        "############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "def PCC_prediction(yhat_4,test_y,s):\n",
        " \n",
        "  test_o=test_y.reshape((s,5))\n",
        "  yhat=yhat_4.reshape((s,5))\n",
        "  \n",
        "  \n",
        "  \n",
        "  y_1_no=yhat[:,0]\n",
        "  y_2_no=yhat[:,1]\n",
        "  y_3_no=yhat[:,2]\n",
        "  y_4_no=yhat[:,3]\n",
        "  y_5_no=yhat[:,4]\n",
        "  # y_6_no=yhat[:,5]\n",
        "  # y_7_no=yhat[:,6]\n",
        "  \n",
        "  \n",
        "  y_test_1=test_o[:,0]\n",
        "  y_test_2=test_o[:,1]\n",
        "  y_test_3=test_o[:,2]\n",
        "  y_test_4=test_o[:,3]\n",
        "  y_test_5=test_o[:,4]\n",
        "  # y_test_6=test_o[:,5]\n",
        "  # y_test_7=test_o[:,6]\n",
        "  \n",
        "  \n",
        "  \n",
        "  cutoff=6\n",
        "  fs=200\n",
        "  order=4\n",
        "  \n",
        "  nyq = 0.5 * fs\n",
        "  ## filtering data ##\n",
        "  def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "      normal_cutoff = cutoff / nyq\n",
        "      # Get the filter coefficients \n",
        "      b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "      y = filtfilt(b, a, data)\n",
        "      return y\n",
        "  \n",
        "  \n",
        "  \n",
        "  y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)\n",
        "  y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)\n",
        "  y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)\n",
        "  y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)\n",
        "  y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)\n",
        "  # y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)\n",
        "  # y_7=butter_lowpass_filter(y_7_no, cutoff, fs, order)\n",
        "  \n",
        "  \n",
        "  Y_1=y_1\n",
        "  Y_2=y_2\n",
        "  Y_3=y_3\n",
        "  Y_4=y_4\n",
        "  Y_5=y_5\n",
        "  # Y_6=y_6\n",
        "  # Y_7=y_7\n",
        "  \n",
        "  \n",
        "  ###calculate RMSE\n",
        "  \n",
        "  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100\n",
        "  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100\n",
        "  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100\n",
        "  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100\n",
        "  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100\n",
        "  # rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100\n",
        "  # rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100\n",
        "  \n",
        "  \n",
        "  print(rmse_1)\n",
        "  print(rmse_2)\n",
        "  print(rmse_3)\n",
        "  print(rmse_4)\n",
        "  print(rmse_5)\n",
        "  # print(rmse_6)\n",
        "  # print(rmse_7)\n",
        "  \n",
        "  \n",
        "  p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "  p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "  p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "  p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "  p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "  # p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "  # p_7=np.corrcoef(y_7, y_test_7)[0, 1]\n",
        "  \n",
        "  \n",
        "  print(\"\\n\") \n",
        "  print(p_1)\n",
        "  print(p_2)\n",
        "  print(p_3)\n",
        "  print(p_4)\n",
        "  print(p_5)\n",
        "  # print(p_6)\n",
        "  # print(p_7)\n",
        "  \n",
        "  \n",
        "  \n",
        "              ### Correlation ###\n",
        "  p=np.array([p_1,p_2,p_3,p_4,p_5])\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "      #### Mean and standard deviation ####\n",
        "  \n",
        "  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])\n",
        "  \n",
        "      #### Mean and standard deviation ####\n",
        "  m=statistics.mean(rmse)\n",
        "  SD=statistics.stdev(rmse)\n",
        "  print('Mean: %.3f' % m,'+/- %.3f' %SD)\n",
        "   \n",
        "  m_c=statistics.mean(p)\n",
        "  SD_c=statistics.stdev(p)\n",
        "  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n",
        "  \n",
        "  \n",
        "  return rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5\n",
        "  \n",
        "\n",
        "############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "def estimate_coef(x, y):\n",
        "    # number of observations/points\n",
        "    n = np.size(x)\n",
        "  \n",
        "    # mean of x and y vector\n",
        "    m_x = np.mean(x)\n",
        "    m_y = np.mean(y)\n",
        "  \n",
        "    # calculating cross-deviation and deviation about x\n",
        "    SS_xy = np.sum(y*x) - n*m_y*m_x\n",
        "    SS_xx = np.sum(x*x) - n*m_x*m_x\n",
        "  \n",
        "    # calculating regression coefficients\n",
        "    b_1 = SS_xy / SS_xx\n",
        "    b_0 = m_y - b_1*m_x\n",
        "  \n",
        "    return (b_0, b_1)  \n",
        "\n",
        "\n",
        "############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "def DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5):\n",
        "\n",
        "  a_1,b_1=estimate_coef(Y_1,Z_1)\n",
        "  a_2,b_2=estimate_coef(Y_2,Z_2)\n",
        "  a_3,b_3=estimate_coef(Y_3,Z_3)\n",
        "  a_4,b_4=estimate_coef(Y_4,Z_4)\n",
        "  a_5,b_5=estimate_coef(Y_5,Z_5)\n",
        "  \n",
        "  #### All 16 angles prediction  ####\n",
        "  \n",
        "   \n",
        "  test_o=test_y.reshape((s,5))\n",
        "  yhat=yhat_4.reshape((s,5))\n",
        "  \n",
        "  \n",
        "  y_1_no=yhat[:,0]\n",
        "  y_2_no=yhat[:,1]\n",
        "  y_3_no=yhat[:,2]\n",
        "  y_4_no=yhat[:,3]\n",
        "  y_5_no=yhat[:,4]\n",
        "  # y_6_no=yhat[:,5]\n",
        "  # y_7_no=yhat[:,6]\n",
        "  \n",
        "  \n",
        "  \n",
        "  y_test_1=test_o[:,0]\n",
        "  y_test_2=test_o[:,1]\n",
        "  y_test_3=test_o[:,2]\n",
        "  y_test_4=test_o[:,3]\n",
        "  y_test_5=test_o[:,4]\n",
        "  # y_test_6=test_o[:,5]\n",
        "  # y_test_7=test_o[:,6]\n",
        "  \n",
        "  \n",
        "  cutoff=6\n",
        "  fs=200\n",
        "  order=4\n",
        "  \n",
        "  nyq = 0.5 * fs\n",
        "  ## filtering data ##\n",
        "  def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "      normal_cutoff = cutoff / nyq\n",
        "      # Get the filter coefficients \n",
        "      b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "      y = filtfilt(b, a, data)\n",
        "      return y\n",
        "  \n",
        "  \n",
        "  \n",
        "  y_1=butter_lowpass_filter(y_1_no, cutoff, fs, order)\n",
        "  y_2=butter_lowpass_filter(y_2_no, cutoff, fs, order)\n",
        "  y_3=butter_lowpass_filter(y_3_no, cutoff, fs, order)\n",
        "  y_4=butter_lowpass_filter(y_4_no, cutoff, fs, order)\n",
        "  y_5=butter_lowpass_filter(y_5_no, cutoff, fs, order)\n",
        "  # y_6=butter_lowpass_filter(y_6_no, cutoff, fs, order)\n",
        "  # y_7=butter_lowpass_filter(y_7_no, cutoff, fs, order)\n",
        "  \n",
        "  \n",
        "  \n",
        "  y_1=y_1*b_1+a_1\n",
        "  y_2=y_2*b_2+a_2\n",
        "  y_3=y_3*b_3+a_3\n",
        "  y_4=y_4*b_4+a_4\n",
        "  y_5=y_5*b_5+a_5\n",
        "  # y_6=y_6*b_6+a_6\n",
        "  # y_7=y_7*b_7+a_7\n",
        "  \n",
        "  \n",
        "  ###calculate RMSE\n",
        "  \n",
        "  rmse_1 =((np.sqrt(mean_squared_error(y_test_1,y_1)))/(max(y_test_1)-min(y_test_1)))*100\n",
        "  rmse_2 =((np.sqrt(mean_squared_error(y_test_2,y_2)))/(max(y_test_2)-min(y_test_2)))*100\n",
        "  rmse_3 =((np.sqrt(mean_squared_error(y_test_3,y_3)))/(max(y_test_3)-min(y_test_3)))*100\n",
        "  rmse_4 =((np.sqrt(mean_squared_error(y_test_4,y_4)))/(max(y_test_4)-min(y_test_4)))*100\n",
        "  rmse_5 =((np.sqrt(mean_squared_error(y_test_5,y_5)))/(max(y_test_5)-min(y_test_5)))*100\n",
        "  # rmse_6 =((np.sqrt(mean_squared_error(y_test_6,y_6)))/(max(y_test_6)-min(y_test_6)))*100\n",
        "  # rmse_7 =((np.sqrt(mean_squared_error(y_test_7,y_7)))/(max(y_test_7)-min(y_test_7)))*100\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  print(rmse_1)\n",
        "  print(rmse_2)\n",
        "  print(rmse_3)\n",
        "  print(rmse_4)\n",
        "  print(rmse_5)\n",
        "  # print(rmse_6)\n",
        "  # print(rmse_7)\n",
        "  \n",
        "  \n",
        "  \n",
        "  p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "  p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "  p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "  p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "  p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "  # p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "  # p_7=np.corrcoef(y_7, y_test_7)[0, 1]\n",
        "  \n",
        "  \n",
        "  print(\"\\n\") \n",
        "  print(p_1)\n",
        "  print(p_2)\n",
        "  print(p_3)\n",
        "  print(p_4)\n",
        "  print(p_5)\n",
        "  # print(p_6)\n",
        "  # print(p_7)\n",
        "  \n",
        "  \n",
        "              ### Correlation ###\n",
        "  p=np.array([p_1,p_2,p_3,p_4,p_5])\n",
        "  \n",
        "  \n",
        "  \n",
        "      #### Mean and standard deviation ####\n",
        "  \n",
        "  rmse=np.array([rmse_1,rmse_2,rmse_3,rmse_4,rmse_5])\n",
        "  \n",
        "      #### Mean and standard deviation ####\n",
        "  m=statistics.mean(rmse)\n",
        "  SD=statistics.stdev(rmse)\n",
        "  print('Mean: %.3f' % m,'+/- %.3f' %SD)\n",
        "   \n",
        "  m_c=statistics.mean(p)\n",
        "  SD_c=statistics.stdev(p)\n",
        "  print('Mean: %.3f' % m_c,'+/- %.3f' %SD_c)\n",
        "  \n",
        "  return rmse, p\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n"
      ],
      "metadata": {
        "id": "RdVunLKprSkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function of Base Models"
      ],
      "metadata": {
        "id": "w3vZzWNAqIAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU-Net"
      ],
      "metadata": {
        "id": "0KOK9cyZDE2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################### 1. GRU-Net  #################################################\n",
        "\n",
        "\n",
        "num_pred=5\n",
        "\n",
        "def GRU_Net(inputs_1D_N,inputs_2D_N):\n",
        "  \n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "\n",
        "  return (output_GRU)  \n",
        "  \n",
        "def GRU_Net_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)  \n",
        "  return (output_GRU)\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "PhjzUZ5KDHJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv2D-Net"
      ],
      "metadata": {
        "id": "ehBHDBxWEOiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #############################################################################################################################################\n",
        "########################  2. Conv2D-Net  ####################################################################################################\n",
        "  \n",
        "def Conv2D_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "\n",
        "  return (output_C2)\n",
        "  \n",
        "def Conv2D_Net_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001),activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "\n",
        "  \n",
        "  return (output_C2)  \n",
        "  "
      ],
      "metadata": {
        "id": "1ExlNYL7qCkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv1D-Net"
      ],
      "metadata": {
        "id": "3VzxRd8oCQGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################################################################################################################\n",
        "#########################################################  3. Conv1D-Net  ######################################################################################################\n",
        "\n",
        "def Conv1D_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "\n",
        "  return (output_C1)\n",
        "\n",
        "  \n",
        "  \n",
        "def Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "  return (output_C1)  \n",
        "  \n",
        "####################################################################################################################################################################################  \n"
      ],
      "metadata": {
        "id": "JkY9lPWaCS0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU-Conv2D-Net"
      ],
      "metadata": {
        "id": "XWTLaNX-c7MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N):\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  X=concatenate([X,model_2])\n",
        "\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  \n",
        "  return (output_C2)  \n",
        "  \n",
        "def GRU_Conv2D_Net_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  X=concatenate([X,model_2])\n",
        "\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001),activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "\n",
        "  return (output_C2)\n",
        "  "
      ],
      "metadata": {
        "id": "QKXVhp9-c_3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU-Conv1D-Net"
      ],
      "metadata": {
        "id": "UMuPBjqBSgOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N):\n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "\n",
        "  return (output_C1)\n",
        "\n",
        "  \n",
        "def GRU_Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "  return (output_C1)  \n",
        "  "
      ],
      "metadata": {
        "id": "7LABiU90SgOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv2D-Conv1D-Net"
      ],
      "metadata": {
        "id": "eW-aN7OCig6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Conv2D_Conv1D_Net(inputs_1D_N,inputs_2D_N):\n",
        "  \n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,X])\n",
        "\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "\n",
        "  return (output_C1)\n",
        "\n",
        "  \n",
        "def Conv2D_Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,X])\n",
        "\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "  return (output_C1)  \n",
        "  "
      ],
      "metadata": {
        "id": "ohZTMTHfig6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-Sub-Net-1"
      ],
      "metadata": {
        "id": "3BeIm4icl5vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################################################################################################################  \n",
        "####################################################   10. Kinetics-Net   ####################################################################\n",
        "\n",
        "def Kinetics_Sub_Net_1(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  output = Average()([output_GRU,output_C2])\n",
        "\n",
        "  return (output_C2,output_GRU,output)\n",
        "\n",
        "  \n",
        "def Kinetics_Sub_Net_1_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "  output = Average()([output_GRU,output_C2])\n",
        "  \n",
        "  return (output_C2,output_GRU,output)\n"
      ],
      "metadata": {
        "id": "Q07wX9z9l-7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-Sub-Net-2"
      ],
      "metadata": {
        "id": "mp511ysezNer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################################################################################################################  \n",
        "####################################################   10. Kinetics-Net   ####################################################################\n",
        "\n",
        "def Kinetics_Sub_Net_2(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  output = Average()([output_GRU,output_C1])\n",
        "\n",
        "  return (output_C1,output_GRU,output)\n",
        "\n",
        "  \n",
        "def Kinetics_Sub_Net_2_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "  output = Average()([output_GRU,output_C1])\n",
        "  \n",
        "  return (output_C1,output_GRU,output)\n"
      ],
      "metadata": {
        "id": "lmCI8OauzNew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-Sub-Net-3"
      ],
      "metadata": {
        "id": "qKB1OLSDUXHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################################################################################################################  \n",
        "####################################################   10. Kinetics-Net   ####################################################################\n",
        "\n",
        "def Kinetics_Sub_Net_3(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  output = Average()([output_C2,output_C1])\n",
        "\n",
        "  return (output_C1,output_C2,output)\n",
        "\n",
        "  \n",
        "def Kinetics_Sub_Net_3_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "  output = Average()([output_C2,output_C1])\n",
        "  \n",
        "  return (output_C1,output_C2,output)\n"
      ],
      "metadata": {
        "id": "r6MfNMlAUXIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-Net"
      ],
      "metadata": {
        "id": "cUlxCOX30TTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################################################################################################################  \n",
        "####################################################   10. Kinetics-Net   ####################################################################\n",
        "\n",
        "def Kinetics_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  output = Average()([output_C2,output_C1,output_GRU])\n",
        "\n",
        "  return (output_C1,output_C2,output_GRU,output)\n",
        "\n",
        "  \n",
        "def Kinetics_Net_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "  \n",
        "  \n",
        "  output = Average()([output_C2,output_C1,output_GRU])\n",
        "  \n",
        "  return (output_C1,output_C2,output_GRU,output)\n"
      ],
      "metadata": {
        "id": "rv6EMjoi0TTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-FM-Net"
      ],
      "metadata": {
        "id": "64d1d4-TAYfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################################################################################################################  \n",
        "####################################################   10. Kinetics-Net   ####################################################################\n",
        "\n",
        "def Kinetics_FM_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.3)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.1)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "\n",
        "  output_GRU_1=Dense(128,activation='relu')(output_GRU)\n",
        "  output_GRU_1=Dense(num_pred,activation='sigmoid')(output_GRU_1)\n",
        "  #output_GRU_1=Dense(1,activation='sigmoid')(output_GRU_1)\n",
        "  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])\n",
        "  \n",
        "  output_C2_1=Dense(128,activation='relu')(output_C2)\n",
        "  output_C2_1=Dense(num_pred,activation='sigmoid')(output_C2_1)\n",
        "  #output_C2_1=Dense(1,activation='sigmoid')(output_C2_1)\n",
        "  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])\n",
        "\n",
        "  output_C1_1=Dense(128,activation='relu')(output_C1)\n",
        "  output_C1_1=Dense(num_pred,activation='sigmoid')(output_C1_1)\n",
        "  #output_C1_1=Dense(1,activation='sigmoid')(output_C1_1)\n",
        "  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])\n",
        "  \n",
        "\n",
        "  weight=output_GRU_1+output_C2_1+output_C1_1\n",
        "  \n",
        "  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])\n",
        "  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])\n",
        "  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])\n",
        "  \n",
        "\n",
        "  output = output_GRU_2+output_C1_2+output_C2_2\n",
        "  \n",
        "\n",
        "  return (output_C1,output_C2,output_GRU,output)\n",
        "\n",
        "  \n",
        "def Kinetics_FM_Net_pcc(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_1)\n",
        "  model_1=Dropout(0.25)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "  \n",
        "  model_2=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_2)\n",
        "  model_2=Dropout(0.3)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(256, (5, 3), activation='relu',padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "  X=Conv2D(512, (5, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 1))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  # X=Dropout(0.05)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "  X=concatenate([X,model_2])\n",
        "  \n",
        "  \n",
        "  model_3=GRU(512,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True)(inputs_1D_N)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=GRU(256,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True)(model_3)\n",
        "  model_3=Dropout(0.3)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=256, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=512, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  # CNN=Dropout(0.05)(CNN)\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  CNN=concatenate([CNN,model_3])\n",
        "\n",
        "  output_GRU=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_GRU=Reshape(target_shape=(w,num_pred))(output_GRU)\n",
        "  output_C2=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C2=Reshape(target_shape=(w,num_pred))(output_C2)\n",
        "  output_C1=Dense(num_pred*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "  output_C1=Reshape(target_shape=(w,num_pred))(output_C1)\n",
        "\n",
        "  \n",
        "  output_GRU_1=Dense(128,activation='relu')(output_GRU)\n",
        "  output_GRU_1=Dropout(0.4)(output_GRU_1)\n",
        "  #output_GRU_1=Dense(64,activation='relu')(output_GRU_1)\n",
        "  output_GRU_1=Dense(num_pred,activation='sigmoid')(output_GRU_1)\n",
        "  #output_GRU_1=Dense(1,activation='sigmoid')(output_GRU_1)\n",
        "  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])\n",
        "  \n",
        "  output_C2_1=Dense(128,activation='relu')(output_C2)\n",
        "  output_C2_1=Dropout(0.4)(output_C2_1)\n",
        "  #output_C2_1=Dense(64,activation='relu')(output_C2_1)\n",
        "  output_C2_1=Dense(num_pred,activation='sigmoid')(output_C2_1)\n",
        "  #output_C2_1=Dense(1,activation='sigmoid')(output_C2_1)\n",
        "  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])\n",
        "\n",
        "  output_C1_1=Dense(128,activation='relu')(output_C1)\n",
        "  output_C1_1=Dropout(0.4)(output_C1_1)\n",
        "  #output_C1_1=Dense(64,activation='relu')(output_C1_1)\n",
        "  output_C1_1=Dense(num_pred,activation='sigmoid')(output_C1_1)\n",
        "  #output_C1_1=Dense(1,activation='sigmoid')(output_C1_1)\n",
        "  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])\n",
        "  \n",
        "\n",
        "  weight=output_GRU_1+output_C2_1+output_C1_1\n",
        "  \n",
        "  output_GRU_2 = tf.keras.layers.Multiply()([output_GRU, output_GRU_1])\n",
        "  output_C2_2 = tf.keras.layers.Multiply()([output_C2, output_C2_1])\n",
        "  output_C1_2 = tf.keras.layers.Multiply()([output_C1, output_C1_1])\n",
        "  \n",
        "  output = output_GRU_2+output_C1_2+output_C2_2\n",
        "    \n",
        "  return (output_C1,output_C2,output_GRU,output)\n"
      ],
      "metadata": {
        "id": "rnnHxYu3AYfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "ijs84QJArnfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping"
      ],
      "metadata": {
        "id": "0SVdqu0G299l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_early_stopping=tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=15,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ") \n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "o7ldf5owoELE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU-Net"
      ],
      "metadata": {
        "id": "v1_UM2Co_ghM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRU-NET ###\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_1 = tf.keras.Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True, callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_1.save(path+'model_GRU.h5')\n",
        "print('>Saved %s' % path)\n",
        "\n",
        "############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "yhat_4= model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "s=test_X_1D.shape[0]*w\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_GRU=rmse\n",
        "PCC_GRU=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "9YusGcDjrqFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GRU-NET ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Net_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "# opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "model_2.save(path+'model_GRU_PCC.h5')\n",
        "\n",
        "\n",
        "\n",
        "############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "yhat_4 = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "RMSE_GRU_DLR=rmse\n",
        "PCC_GRU_DLR=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "ryd7vO8uvg4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GRU-NET ###\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Net(inputs_1D_N,inputs_2D_N)\n",
        "model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_3.save(path+'model_GRU_JL.h5')\n",
        "\n",
        "\n",
        "############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "yhat_4= model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "RMSE_GRU_JL=rmse\n",
        "PCC_GRU_JL=p\n",
        "\n",
        "\n",
        "\n",
        "ablation_1=np.hstack([RMSE_GRU,PCC_GRU])\n",
        "ablation_2=np.hstack([RMSE_GRU_DLR,PCC_GRU_DLR])\n",
        "ablation_3=np.hstack([RMSE_GRU_JL,PCC_GRU_JL])\n",
        "\n",
        "GRU_result=np.vstack([ablation_1,ablation_2,ablation_3])"
      ],
      "metadata": {
        "id": "zkZ64jXLyt52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv2D-Net"
      ],
      "metadata": {
        "id": "LInfrA4Y_mnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Conv2D-NET ###\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=Conv2D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "\n",
        "model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2,\\\n",
        "                    shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_1.save(path+'model_Conv2D.h5')\n",
        "\n",
        "\n",
        "############################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "yhat_4 = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Conv2D=rmse\n",
        "PCC_Conv2D=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "jOjVtVwZy12f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Conv2D_Net-NET ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=Conv2D_Net_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, \\\n",
        "                    shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "model_2.save(path+'model_Conv2D_PCC.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "yhat_4 = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "\n",
        "RMSE_Conv2D_DLR=rmse\n",
        "PCC_Conv2D_DLR=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "CLATB1rzH6xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Conv2D-NET ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=Conv2D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2,\\\n",
        "                    shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_3.save(path+'model_Conv2D_JL.h5')\n",
        "\n",
        "yhat_4= model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Conv2D_JL=rmse\n",
        "PCC_Conv2D_JL=p\n",
        "\n",
        "ablation_4=np.hstack([RMSE_Conv2D,PCC_Conv2D])\n",
        "ablation_5=np.hstack([RMSE_Conv2D_DLR,PCC_Conv2D_DLR])\n",
        "ablation_6=np.hstack([RMSE_Conv2D_JL,PCC_Conv2D_JL])\n",
        "\n",
        "Conv2D_result=np.vstack([ablation_4,ablation_5,ablation_6])"
      ],
      "metadata": {
        "id": "CVWwEhC1H_Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv1D-Net"
      ],
      "metadata": {
        "id": "K_iqxewuIGrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Conv1D-Net, Conv1D-DLR-Net, Conv1D-JL-Net\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "### Conv1D-NET ###\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=Conv1D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2,\\\n",
        "                    shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_1.save(path+'model_Conv1D.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "yhat_4 = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Conv1D=rmse\n",
        "PCC_Conv1D=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "IgO5wyt9KTKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Conv1D-NET ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2,\\\n",
        "                    shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_2.save(path+'model_Conv1D_PCC.h5')\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "yhat_4 = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "\n",
        "RMSE_Conv1D_DLR=rmse\n",
        "PCC_Conv1D_DLR=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "lzhPEjddIJn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Conv1D-NET ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=Conv1D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation),\\\n",
        "                    verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_3.save(path+'model_Conv1D_JL.h5')\n",
        "\n",
        "\n",
        "yhat_4= model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "RMSE_Conv1D_JL=rmse\n",
        "PCC_Conv1D_JL=p\n",
        "\n",
        "ablation_7=np.hstack([RMSE_Conv1D,PCC_Conv1D])\n",
        "ablation_8=np.hstack([RMSE_Conv1D_DLR,PCC_Conv1D_DLR])\n",
        "ablation_9=np.hstack([RMSE_Conv1D_JL,PCC_Conv1D_JL])\n",
        "\n",
        "Conv1D_result=np.vstack([ablation_7,ablation_8,ablation_9])"
      ],
      "metadata": {
        "id": "p4ugWcf8KOOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU-Conv2D-Net"
      ],
      "metadata": {
        "id": "cYVyswqtKfU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRU-Conv2D-Net ###\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_1.save(path+'model_GRU_Conv2D.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "yhat_4 = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "RMSE_GRU_Conv2D=rmse\n",
        "PCC_GRU_Conv2D=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "tQg1wIOLKhbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GRU-Conv2D-DLR-Net ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Conv2D_Net_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_2.save(path+'model_GRU_Conv2D_PCC.h5')\n",
        "\n",
        "\n",
        "\n",
        "yhat_4 = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "\n",
        "RMSE_GRU_Conv2D_DLR=rmse\n",
        "PCC_GRU_Conv2D_DLR=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "L7m2CEaKKjvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################   GRU-Conv2D-JL-Net    #####################\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_3.save(path+'model_GRU_Conv2D_JL.h5')\n",
        "\n",
        "\n",
        "\n",
        "yhat_4= model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_GRU_Conv2D_JL=rmse\n",
        "PCC_GRU_Conv2D_JL=p\n",
        "\n",
        "ablation_10=np.hstack([RMSE_GRU_Conv2D,PCC_GRU_Conv2D])\n",
        "ablation_11=np.hstack([RMSE_GRU_Conv2D_DLR,PCC_GRU_Conv2D_DLR])\n",
        "ablation_12=np.hstack([RMSE_GRU_Conv2D_JL,PCC_GRU_Conv2D_JL])\n",
        "\n",
        "GRU_Conv2D_result=np.vstack([ablation_10,ablation_11,ablation_12])"
      ],
      "metadata": {
        "id": "HLDQ4RhxKl-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU-Conv1D-Net\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "69tnkoIfjNvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRU-Conv1D-Net ###\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_1.save(path+'model_GRU_Conv1D.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "yhat_4 = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "RMSE_GRU_Conv1D=rmse\n",
        "PCC_GRU_Conv1D=p\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "5t7ZaaBKjjIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GRU-Conv1D-DLR-Net ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "model_2.save(path+'model_GRU_Conv1D_PCC.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "yhat_4 = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "RMSE_GRU_Conv1D_DLR=rmse\n",
        "PCC_GRU_Conv1D_DLR=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "e4YHpJeZjsts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################   GRU-Conv1D-JL-Net    #####################\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_3.save(path+'model_GRU_Conv1D_JL.h5')\n",
        "\n",
        "\n",
        "\n",
        "yhat_4= model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "RMSE_GRU_Conv1D_JL=rmse\n",
        "PCC_GRU_Conv1D_JL=p\n",
        "\n",
        "ablation_13=np.hstack([RMSE_GRU_Conv1D,PCC_GRU_Conv1D])\n",
        "ablation_14=np.hstack([RMSE_GRU_Conv1D_DLR,PCC_GRU_Conv1D_DLR])\n",
        "ablation_15=np.hstack([RMSE_GRU_Conv1D_JL,PCC_GRU_Conv1D_JL])\n",
        "\n",
        "GRU_Conv1D_result=np.vstack([ablation_13,ablation_14,ablation_15])"
      ],
      "metadata": {
        "id": "0PxpcBPrjxMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv2D-Conv1D-Net"
      ],
      "metadata": {
        "id": "05XaGyCJjx5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Conv2D-Conv1D-Net ###\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=Conv2D_Conv1D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_1.save(path+'model_Conv2D_Conv1D.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "yhat_4 = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Conv2D_Conv1D=rmse\n",
        "PCC_Conv2D_Conv1D=p\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "4-C_Ea-Vj3Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Conv2D-Conv1D-DLR-Net ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=Conv2D_Conv1D_Net_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], train_y_5, epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_2.save(path+'model_Conv2D_Conv1D_PCC.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "yhat_4 = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "RMSE_Conv2D_Conv1D_DLR=rmse\n",
        "PCC_Conv2D_Conv1D_DLR=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "ZYSg5Fr9j59B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################   Conv2D-Conv1D-JL-Net    #####################\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_3 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], train_y_5, epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], Y_validation), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_3.save(path+'model_Conv2D_Conv1D_JL.h5')\n",
        "\n",
        "\n",
        "\n",
        "yhat_4= model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Conv2D_Conv1D_JL=rmse\n",
        "PCC_Conv2D_Conv1D_JL=p\n",
        "\n",
        "ablation_16=np.hstack([RMSE_Conv2D_Conv1D,PCC_Conv2D_Conv1D])\n",
        "ablation_17=np.hstack([RMSE_Conv2D_Conv1D_DLR,PCC_Conv2D_Conv1D_DLR])\n",
        "ablation_18=np.hstack([RMSE_Conv2D_Conv1D_JL,PCC_Conv2D_Conv1D_JL])\n",
        "\n",
        "Conv2D_Conv1D_result=np.vstack([ablation_16,ablation_17,ablation_18])"
      ],
      "metadata": {
        "id": "hidgHEitj-dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-Sub-Net-1"
      ],
      "metadata": {
        "id": "Gz05NtTOkO2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Sub-Net-1 ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Kinetics_Sub_Net_1(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_1= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "model_1.save(path+'model_Kinetics_Sub_1.h5')\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "[yhat_1,yhat_2,yhat_4] = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "\n",
        "RMSE_Kinetics_Sub_Net_1=rmse\n",
        "PCC_Kinetics_Sub_Net_1=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "642egWYRkSNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Sub-DLR-Net-1 ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Kinetics_Sub_Net_1_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "\n",
        "\n",
        "model_2= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_2.save(path+'model_Kinetics_Sub_1_PCC.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "[yhat_1,yhat_2,yhat_4] = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_Sub_DLR_Net_1=rmse\n",
        "PCC_Kinetics_Sub_DLR_Net_1=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "cOLuofIykVdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Sub-JL-Net-1 ###\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Kinetics_Sub_Net_1(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "\n",
        "\n",
        "model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_3.save(path+'model_Kinetics_Sub_1_JL.h5')\n",
        "\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "\n",
        "[yhat_1,yhat_2,yhat_4]= model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_Sub_JL_Net_1=rmse\n",
        "PCC_Kinetics_Sub_JL_Net_1=p\n",
        "\n",
        "ablation_19=np.hstack([RMSE_Kinetics_Sub_Net_1,PCC_Kinetics_Sub_Net_1])\n",
        "ablation_20=np.hstack([RMSE_Kinetics_Sub_DLR_Net_1,PCC_Kinetics_Sub_DLR_Net_1])\n",
        "ablation_21=np.hstack([RMSE_Kinetics_Sub_JL_Net_1,PCC_Kinetics_Sub_JL_Net_1])\n",
        "\n",
        "Kinetics_Sub_Net_1_result=np.vstack([ablation_19,ablation_20,ablation_21])"
      ],
      "metadata": {
        "id": "8XKkaSn2kZHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-Sub-Net-2"
      ],
      "metadata": {
        "id": "w-MM62SMkjvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Sub-Net-2 ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Kinetics_Sub_Net_2(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_1= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "# # # summarize history for loss\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "model_1.save(path+'model_Kinetics_Sub_2.h5')\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "[yhat_1,yhat_2,yhat_4] = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_Sub_Net_2=rmse\n",
        "PCC_Kinetics_Sub_Net_2=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "5yg3eRRHklrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Sub-DLR-Net-1 ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Kinetics_Sub_Net_2_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "\n",
        "\n",
        "model_2= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_2.save(path+'model_Kinetics_Sub_2_PCC.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "[yhat_1,yhat_2,yhat_4] = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_Sub_DLR_Net_2=rmse\n",
        "PCC_Kinetics_Sub_DLR_Net_2=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "B6aX1v_WkoOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Sub-JL-Net-2 ###\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Kinetics_Sub_Net_2(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "\n",
        "\n",
        "model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_3.save(path+'model_Kinetics_Sub_2_JL.h5')\n",
        "\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "\n",
        "[yhat_1,yhat_2,yhat_4]= model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_Sub_JL_Net_2=rmse\n",
        "PCC_Kinetics_Sub_JL_Net_2=p\n",
        "\n",
        "ablation_22=np.hstack([RMSE_Kinetics_Sub_Net_2,PCC_Kinetics_Sub_Net_2])\n",
        "ablation_23=np.hstack([RMSE_Kinetics_Sub_DLR_Net_2,PCC_Kinetics_Sub_DLR_Net_2])\n",
        "ablation_24=np.hstack([RMSE_Kinetics_Sub_JL_Net_2,PCC_Kinetics_Sub_JL_Net_2])\n",
        "\n",
        "Kinetics_Sub_Net_2_result=np.vstack([ablation_22,ablation_23,ablation_24])"
      ],
      "metadata": {
        "id": "9_UpQsdUkr90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-Sub-Net-3"
      ],
      "metadata": {
        "id": "nUvTcZZqksoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Sub-Net-3 ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Kinetics_Sub_Net_3(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_1= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "model_1.save(path+'model_Kinetics_Sub_3.h5')\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "[yhat_1,yhat_2,yhat_4] = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "\n",
        "RMSE_Kinetics_Sub_Net_3=rmse\n",
        "PCC_Kinetics_Sub_Net_3=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "LTjgKPb2ky7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Sub-DLR-Net-3 ###\n",
        "\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Kinetics_Sub_Net_3_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_2= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_2.save(path+'model_Kinetics_Sub_3_PCC.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "[yhat_1,yhat_2,yhat_4] = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_Sub_DLR_Net_3=rmse\n",
        "PCC_Kinetics_Sub_DLR_Net_3=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "G_j1MuEJk1uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Sub-JL-Net-2 ###\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Kinetics_Sub_Net_3(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "\n",
        "model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_3.save(path+'model_Kinetics_Sub_3_JL.h5')\n",
        "\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "\n",
        "[yhat_1,yhat_2,yhat_4]= model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_Sub_JL_Net_3=rmse\n",
        "PCC_Kinetics_Sub_JL_Net_3=p\n",
        "\n",
        "ablation_25=np.hstack([RMSE_Kinetics_Sub_Net_3,PCC_Kinetics_Sub_Net_3])\n",
        "ablation_26=np.hstack([RMSE_Kinetics_Sub_DLR_Net_3,PCC_Kinetics_Sub_DLR_Net_3])\n",
        "ablation_27=np.hstack([RMSE_Kinetics_Sub_JL_Net_3,PCC_Kinetics_Sub_JL_Net_3])\n",
        "\n",
        "Kinetics_Sub_Net_3_result=np.vstack([ablation_25,ablation_26,ablation_27])"
      ],
      "metadata": {
        "id": "nkjy1hHYk1yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-Net"
      ],
      "metadata": {
        "id": "LsK0kmPxlBRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Kinetics-Net ###\n",
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Kinetics_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "\n",
        "model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_1.save(path+'model_Kinetics.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4] = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "RMSE_Kinetics=rmse\n",
        "PCC_Kinetics=p"
      ],
      "metadata": {
        "id": "UQZOrv3JlDev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Kinetics_Net_pcc(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=50, batch_size=64, validation_data=([X_validation_1D,X_validation_2D],\\\n",
        "                                                                            [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "model_2.save(path+'model_Kinetics_PCC.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4] = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_DLR=rmse\n",
        "PCC_Kinetics_DLR=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "LPTvHgrflKGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  \n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Kinetics_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer=opt, metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_3.save(path+'model_Kinetics_JL.h5')\n",
        "\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4] = model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_JL=rmse\n",
        "PCC_Kinetics_JL=p\n",
        "\n",
        "ablation_28=np.hstack([RMSE_Kinetics,PCC_Kinetics])\n",
        "ablation_29=np.hstack([RMSE_Kinetics_DLR,PCC_Kinetics_DLR])\n",
        "ablation_30=np.hstack([RMSE_Kinetics_JL,PCC_Kinetics_JL])\n",
        "\n",
        "Kinetics_result=np.vstack([ablation_28,ablation_29,ablation_30])\n"
      ],
      "metadata": {
        "id": "sWYPKu9JlOtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kinetics-FM-Net"
      ],
      "metadata": {
        "id": "hZYk4vLflZu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_1 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "\n",
        "model_1.compile(loss=correlation_coefficient_loss_1, optimizer=opt, metrics=[correlation_coefficient_loss_1])\n",
        "\n",
        "\n",
        "history=model_1.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_1.save(path+'model_Kinetics_FM.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4] = model_1.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "RMSE_Kinetics_FM=rmse\n",
        "PCC_Kinetics_FM=p"
      ],
      "metadata": {
        "id": "mqIJ5H1rle17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output_1,output_2,output_3,output=Kinetics_FM_Net_pcc(inputs_1D_N,inputs_2D_N)\n",
        "model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "\n",
        "\n",
        "model_2.compile(loss=correlation_coefficient_loss, optimizer=opt, metrics=[correlation_coefficient_loss])\n",
        "\n",
        "\n",
        "history=model_2.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=55, batch_size=64, validation_data=([X_validation_1D,X_validation_2D],\\\n",
        "                                                                                        [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "model_2.save(path+'model_Kinetics_FM_PCC.h5')\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4] = model_2.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse, p, Y_1,Y_2,Y_3,Y_4,Y_5=PCC_prediction(yhat_4,test_y,s)\n",
        "rmse,p=DLR_prediction(yhat_4,test_y,s,Y_1,Y_2,Y_3,Y_4,Y_5,Z_1,Z_2,Z_3,Z_4,Z_5)\n",
        "\n",
        "\n",
        "RMSE_Kinetics_FM_DLR=rmse\n",
        "PCC_Kinetics_FM_DLR=p\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "YOtDK4GGlhUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,18) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,3) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Kinetics_FM_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model_3= Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "\n",
        "model_3.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])\n",
        "\n",
        "\n",
        "history=model_3.fit([train_X_1D,train_X_2D], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=40, batch_size=64, validation_data=([X_validation_1D,X_validation_2D], \\\n",
        "                                                                                          [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=True,callbacks=[custom_early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_3.save(path+'model_Kinetics_FM_JL.h5')\n",
        "\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#### All 16 angles prediction  ####\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4] = model_3.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "rmse,p,Z_1,Z_2,Z_3,Z_4,Z_5=RMSE_prediction(yhat_4,test_y,s)\n",
        "\n",
        "RMSE_Kinetics_FM_JL=rmse\n",
        "PCC_Kinetics_FM_JL=p\n",
        "\n",
        "ablation_31=np.hstack([RMSE_Kinetics_FM,PCC_Kinetics_FM])\n",
        "ablation_32=np.hstack([RMSE_Kinetics_FM_DLR,PCC_Kinetics_FM_DLR])\n",
        "ablation_33=np.hstack([RMSE_Kinetics_FM_JL,PCC_Kinetics_FM_JL])\n",
        "\n",
        "Kinetics_FM_result=np.vstack([ablation_31,ablation_32,ablation_33])\n"
      ],
      "metadata": {
        "id": "IfOFgPG8lktX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result Summary"
      ],
      "metadata": {
        "id": "2EOcTsh7llrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "All_result=np.vstack([GRU_result,Conv2D_result,Conv1D_result,GRU_Conv2D_result,GRU_Conv1D_result,Conv2D_Conv1D_result,Kinetics_Sub_Net_1_result,Kinetics_Sub_Net_2_result,Kinetics_Sub_Net_3_result,Kinetics_result,Kinetics_FM_result])\n",
        "\n",
        "from numpy import savetxt\n",
        "\n",
        "savetxt(path+subject+'_All_results.csv', All_result, delimiter=',')"
      ],
      "metadata": {
        "id": "wpQ3c0nLloSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}